# -*- coding: utf-8 -*-
"""Group Project_Group3 (TC3213).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ocf6N6ldDr1qtY_QgxURuFavPJ2-ON2Z

**<h2>Using BeautifulSoup libraries to scrape the website</h2>**


*   The website that we used to scrpe: TrustPilot
*   The focus to scrape fromt the website: Car Dealer
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

# Initialize a DataFrame
df = pd.DataFrame(columns=['Company Name', 'Trust Score', 'Total Review', 'Location'])

# Define the number of pages to scrape
num_pages = 9  # Adjust the number of pages based on your requirement

# Define headers to mimic a web browser
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# Define the maximum number of retries
max_retries = 3

# Loop through each page
for page_num in range(1, num_pages + 1):
    url = f'https://www.trustpilot.com/categories/car_dealer?page={page_num}'

    retry_count = 0

    while retry_count < max_retries:
        # Add headers to the request
        page = requests.get(url, headers=headers)

        # Add a delay between requests to avoid rate limiting
        time.sleep(0.1)  # Adjust the delay as needed

        # Check if the request was successful
        if page.status_code == 200:
            break  # Break out of the retry loop if successful
        elif page.status_code == 403:
            # Increment the retry count
            retry_count += 1
            print(f'Retry {retry_count} - Got 403 Forbidden. Retrying...')
        else:
            print(f'Failed to retrieve data from page {page_num}. Status code: {page.status_code}')
            break  # Break out of the retry loop for other status codes

    if page.status_code == 200:
        soup = BeautifulSoup(page.text, 'html.parser')

        # Find all <div> elements with the specified class
        specific_divs = soup.find_all('div', class_='paper_paper__1PY90 paper_outline__lwsUX card_card__lQWDv card_noPadding__D8PcU styles_wrapper__2JOo2')

        # Iterate through each specific <div> and extract information
        for div in specific_divs:
            # Extracting company information
            company_name_element = div.find('p', class_='typography_heading-xs__jSwUz typography_appearance-default__AAY17 styles_displayName__GOhL2')
            company_name = company_name_element.text.strip() if company_name_element else None

            # Extracting trust_score
            trust_score_element = div.find('span', class_='styles_trustScore__8emxJ')
            trust_score = trust_score_element.text.strip() if trust_score_element else None

            # Extracting total number of review
            total_review_element = div.find('p', class_='styles_ratingText__yQ5S7')
            total_review = total_review_element.text.strip().split('|')[1].replace('reviews', '').strip() if total_review_element else None

            # Extracting location
            location_element = div.find('span', class_='styles_location__ILZb0')
            location = location_element.text.strip() if location_element else None


            # Create a new DataFrame with the current row
            new_row_df = pd.DataFrame({
                'Company Name': [company_name],
                'Trust Score': [trust_score],
                'Total Review': [total_review],
                'Location': [location]
            })

            # Concatenate the new DataFrame to the existing DataFrame
            df = pd.concat([df, new_row_df], ignore_index=True)

        print(f'Page {page_num} scraped successfully.')
    else:
        print(f'Failed to retrieve data from page {page_num} even after retries.')

# Display the DataFrame
df

"""**Result:**


*   From the above we known that a total 163 car dealers are listed in the TrustPilot website.
*   Attributes that we focus on to scrape from it are:


    1. Company Name
    2. Trust Score
    3. Total Review
    4. Location

We then do some simple data cleaning on the **Trust Score** column, which is remove the "Trust Score" and remain the score (float).
"""

df['Trust Score'] = df['Trust Score'].str.extract('(\d+\.\d+)').astype(float)

df

"""We also convert the "Location" column to 2 columns which are the "City" and "Country" which will help us easily to visualize the data later on."""

df[['City', 'Country']] = df['Location'].str.extract('^(.*?),\s*(.*)$').astype(str)
df['City'] = df['City'].apply(lambda x: ''.join(filter(lambda char: char.isprintable(), x)))
df['Country'] = df['Country'].apply(lambda x: ''.join(filter(lambda char: char.isprintable(), x)))

# Drop the original 'Location' column
df.drop(columns=['Location'], inplace=True)

df

"""**Checking for Null Value in each column**"""

# Check for null values in the DataFrame
null_values = df.isnull().sum()

# Display the null values count for each column
print("Null Values in Each Column:")
print(null_values)

"""There is a total of 80 null values in all the columns where 40 null values are from the "Trust Score" column and another 40 null values are from the "Total Review" column."""

# Remove the word 'reviews' from the 'Total Review' column
df['Total Review'] = df['Total Review'].str.replace('review', '').str.strip()

# Fill NaN values with 0
df.fillna(0, inplace=True)

# Check for null values in the DataFrame
null_values = df.isnull().sum()

# Display the null values count for each column
print("Null Values in Each Column:")
print(null_values)

"""We remove the null values and replace it with value 0."""

# Display the cleaned DataFrame
df

"""**Best Company By Trust Score**"""

# Convert 'Trust Score' to numeric (assuming it contains numerical values)
df['Trust Score'] = pd.to_numeric(df['Trust Score'], errors='coerce')

# Sort the DataFrame by 'Trust Score' in descending order
df_sorted = df.sort_values(by='Trust Score', ascending=False)

# Select the top 10 companies
Top_10_companies_by_review = df_sorted.head(10)

# Display the result
Top_10_companies_by_review

"""**Best Company By Review**"""

# Clean 'Total Review' column by removing commas and converting to numeric
df['Total Review'] = pd.to_numeric(df['Total Review'].astype(str).str.replace(',', ''), errors='coerce').astype(int)

# Sort the DataFrame by 'Total Review' in descending order
df_sorted_by_reviews = df.sort_values(by='Total Review', ascending=False)

# Select the top 10 companies
top_10_by_reviews = df_sorted_by_reviews.head(10)

# Display the result
top_10_by_reviews

"""**Deciding on which company should be used in comparing:**

From the two tables shows above, we sort the data by using company review and total review respectively. It is because that some of the company with just 100+ or lower of data by with a high TrustScore. In this case, it will lead the company's review to bias data.

Hence, we decide to use the data which at least 1000 or more review and also the trust score is above 4.5 as our results to compare.

Rule:
> If (data > 1500 AND trustscore >= 4.8)

Results:


*   We Buy Any Car® USA (49639 datas and 4.9 trustscore)
*   CarBrain (5507 datas and 4.8 trustscore)
*   Easy Auto (1988 datas and 4.8 trustscore)

**<h1>Data Extraction**

**Extract 1500 datas from We Buy Any Car® USA**
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

# Initialize a DataFrame
df_company1 = pd.DataFrame(columns=['Consumer Name', 'Review Rating', 'Review Content', 'Date of Experience', 'Company Name'])

# Define the number of pages to scrape
num_pages = 75  # Adjust the number of pages based on your requirement

# Define headers to mimic a web browser
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# Define the maximum number of retries
max_retries = 2

# Loop through each page
for page_num in range(1, num_pages + 1):
    url = f'https://www.trustpilot.com/review/webuyanycarusa.com?page={page_num}'

    retry_count = 0

    while retry_count < max_retries:
        # Add headers to the request
        page = requests.get(url, headers=headers)

        # Add a delay between requests to avoid rate limiting
        time.sleep(1)  # Adjust the delay as needed

        # Check if the request was successful
        if page.status_code == 200:
            break  # Break out of the retry loop if successful
        elif page.status_code == 403:
            # Increment the retry count
            retry_count += 1
            print(f'Retry {retry_count} - Got 403 Forbidden. Retrying...')
        else:
            print(f'Failed to retrieve data from page {page_num}. Status code: {page.status_code}')
            break  # Break out of the retry loop for other status codes

    if page.status_code == 200:
        soup = BeautifulSoup(page.text, 'html.parser')

        # Find all <div> elements with the specified class
        specific_divs = soup.find_all('div', class_='styles_cardWrapper__LcCPA styles_show__HUXRb styles_reviewCard__9HxJJ')

        # Iterate through each specific <div> and extract information
        for div in specific_divs:
            # Extracting consumer information
            consumer_name_element = div.find('span', class_='typography_heading-xxs__QKBS8')
            consumer_name = consumer_name_element.text.strip() if consumer_name_element else None

            # Extracting review rating
            review_rating_element = div.find('div', class_='star-rating_starRating__4rrcf')
            review_rating = review_rating_element.find('img')['alt'] if review_rating_element and review_rating_element.find('img') else None

            # Extracting review content
            review_content_element = div.find('p', class_='typography_body-l__KUYFJ')
            review_content = review_content_element.text.strip() if review_content_element else None

            # Extracting date of experience
            date_of_experience_element = div.find('p', class_='typography_body-m__xgxZ_')
            date_of_experience = date_of_experience_element.text.strip() if date_of_experience_element else None



            # Adding company name (assuming 'webuyanycarusa.com' in this case)
            company_name = 'webuyanycarusa.com'

            # Create a new DataFrame with the current row
            new_row_df = pd.DataFrame({
                'Consumer Name': [consumer_name],
                'Review Rating': [review_rating],
                'Review Content': [review_content],
                'Date of Experience': [date_of_experience],
                'Company Name': [company_name]
            })

            # Concatenate the new DataFrame to the existing DataFrame
            df_company1 = pd.concat([df_company1, new_row_df], ignore_index=True)

        print(f'Page {page_num} scraped successfully.')
    else:
        print(f'Failed to retrieve data from page {page_num} even after retries.')

# Display the DataFrame
df_company1

"""**Extract 1500 datas from CarBrain**"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

# Initialize a DataFrame
df_company2 = pd.DataFrame(columns=['Consumer Name', 'Review Rating', 'Review Content', 'Date of Experience', 'Company Name'])

# Define the number of pages to scrape
num_pages = 75  # Adjust the number of pages based on your requirement

# Define headers to mimic a web browser
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Safari/537.36'
}

# Define the maximum number of retries
max_retries = 2

# Loop through each page
for page_num in range(1, num_pages + 1):
    url = f'https://www.trustpilot.com/review/carbrain.com?page={page_num}'

    retry_count = 0

    while retry_count < max_retries:
        # Add headers to the request
        page = requests.get(url, headers=headers)

        # Add a delay between requests to avoid rate limiting
        time.sleep(1)  # Adjust the delay as needed

        # Check if the request was successful
        if page.status_code == 200:
            break  # Break out of the retry loop if successful
        elif page.status_code == 403:
            # Increment the retry count
            retry_count += 1
            print(f'Retry {retry_count} - Got 403 Forbidden. Retrying...')
        else:
            print(f'Failed to retrieve data from page {page_num}. Status code: {page.status_code}')
            break  # Break out of the retry loop for other status codes

    if page.status_code == 200:
        soup = BeautifulSoup(page.text, 'html.parser')

        # Find all <div> elements with the specified class
        specific_divs = soup.find_all('div', class_='styles_cardWrapper__LcCPA styles_show__HUXRb styles_reviewCard__9HxJJ')

        # Iterate through each specific <div> and extract information
        for div in specific_divs:
            # Extracting consumer information
            consumer_name_element = div.find('span', class_='typography_heading-xxs__QKBS8')
            consumer_name = consumer_name_element.text.strip() if consumer_name_element else None

            # Extracting review rating
            review_rating_element = div.find('div', class_='star-rating_starRating__4rrcf')
            review_rating = review_rating_element.find('img')['alt'] if review_rating_element and review_rating_element.find('img') else None

            # Extracting review content
            review_content_element = div.find('p', class_='typography_body-l__KUYFJ')
            review_content = review_content_element.text.strip() if review_content_element else None

            # Extracting date of experience
            date_of_experience_element = div.find('p', class_='typography_body-m__xgxZ_')
            date_of_experience = date_of_experience_element.text.strip() if date_of_experience_element else None



            # Adding company name (assuming 'CarBrain.com' in this case)
            company_name = 'CarBrain.com'

            # Create a new DataFrame with the current row
            new_row_df = pd.DataFrame({
                'Consumer Name': [consumer_name],
                'Review Rating': [review_rating],
                'Review Content': [review_content],
                'Date of Experience': [date_of_experience],
                'Company Name': [company_name]
            })

            # Concatenate the new DataFrame to the existing DataFrame
            df_company2 = pd.concat([df_company2, new_row_df], ignore_index=True)

        print(f'Page {page_num} scraped successfully.')
    else:
        print(f'Failed to retrieve data from page {page_num} even after retries.')

# Display the DataFrame
df_company2

"""Because there is some pages fail to extract, thats why we do it another time to extract those few pages only."""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

# Define the specific pages to scrape
pages_to_scrape = [23, 24, 25, 26, 27, 28, 57, 58, 59, 60, 61, 62, 75]  # Adjust the list of pages as needed

# Define headers to mimic a web browser
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Safari/537.36'
}

# Define the maximum number of retries
max_retries = 2

# Loop through each page
for page_num in pages_to_scrape:
    url = f'https://www.trustpilot.com/review/carbrain.com?page={page_num}'

    retry_count = 0

    while retry_count < max_retries:
        # Add headers to the request
        page = requests.get(url, headers=headers)

        # Add a delay between requests to avoid rate limiting
        time.sleep(1)  # Adjust the delay as needed

        # Check if the request was successful
        if page.status_code == 200:
            break  # Break out of the retry loop if successful
        elif page.status_code == 403:
            # Increment the retry count
            retry_count += 1
            print(f'Retry {retry_count} - Got 403 Forbidden. Retrying...')
        else:
            print(f'Failed to retrieve data from page {page_num}. Status code: {page.status_code}')
            break  # Break out of the retry loop for other status codes

    if page.status_code == 200:
        soup = BeautifulSoup(page.text, 'html.parser')

        # Find all <div> elements with the specified class
        specific_divs = soup.find_all('div', class_='styles_cardWrapper__LcCPA styles_show__HUXRb styles_reviewCard__9HxJJ')

        # Iterate through each specific <div> and extract information
        for div in specific_divs:
            # Extracting consumer information
            consumer_name_element = div.find('span', class_='typography_heading-xxs__QKBS8')
            consumer_name = consumer_name_element.text.strip() if consumer_name_element else None

            # Extracting review rating
            review_rating_element = div.find('div', class_='star-rating_starRating__4rrcf')
            review_rating = review_rating_element.find('img')['alt'] if review_rating_element and review_rating_element.find('img') else None

            # Extracting review content
            review_content_element = div.find('p', class_='typography_body-l__KUYFJ')
            review_content = review_content_element.text.strip() if review_content_element else None

            # Extracting date of experience
            date_of_experience_element = div.find('p', class_='typography_body-m__xgxZ_')
            date_of_experience = date_of_experience_element.text.strip() if date_of_experience_element else None



            # Adding company name (assuming 'CarBrain.com' in this case)
            company_name = 'CarBrain.com'

            # Create a new DataFrame with the current row
            new_row_df = pd.DataFrame({
                'Consumer Name': [consumer_name],
                'Review Rating': [review_rating],
                'Review Content': [review_content],
                'Date of Experience': [date_of_experience],
                'Company Name': [company_name]
            })

            # Concatenate the new DataFrame to the existing DataFrame
            df_company2 = pd.concat([df_company2, new_row_df], ignore_index=True)

        print(f'Page {page_num} scraped successfully.')
    else:
        print(f'Failed to retrieve data from page {page_num} even after retries.')

# Display the DataFrame
df_company2

"""**Extract 1500 datas from Easy Auto**"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

# Initialize a DataFrame
df_company3 = pd.DataFrame(columns=['Consumer Name', 'Review Rating', 'Review Content', 'Date of Experience', 'Company Name'])

# Define the number of pages to scrape
num_pages = 75  # Adjust the number of pages based on your requirement

# Define headers to mimic a web browser
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# Define the maximum number of retries
max_retries = 2

# Loop through each page
for page_num in range(1, num_pages + 1):
    url = f'https://www.trustpilot.com/review/easyautoonline.com?page={page_num}'

    retry_count = 0

    while retry_count < max_retries:
        # Add headers to the request
        page = requests.get(url, headers=headers)

        # Add a delay between requests to avoid rate limiting
        time.sleep(1)  # Adjust the delay as needed

        # Check if the request was successful
        if page.status_code == 200:
            break  # Break out of the retry loop if successful
        elif page.status_code == 403:
            # Increment the retry count
            retry_count += 1
            print(f'Retry {retry_count} - Got 403 Forbidden. Retrying...')
        else:
            print(f'Failed to retrieve data from page {page_num}. Status code: {page.status_code}')
            break  # Break out of the retry loop for other status codes

    if page.status_code == 200:
        soup = BeautifulSoup(page.text, 'html.parser')

        # Find all <div> elements with the specified class
        specific_divs = soup.find_all('div', class_='styles_cardWrapper__LcCPA styles_show__HUXRb styles_reviewCard__9HxJJ')

        # Iterate through each specific <div> and extract information
        for div in specific_divs:
            # Extracting consumer information
            consumer_name_element = div.find('span', class_='typography_heading-xxs__QKBS8')
            consumer_name = consumer_name_element.text.strip() if consumer_name_element else None

            # Extracting review rating
            review_rating_element = div.find('div', class_='star-rating_starRating__4rrcf')
            review_rating = review_rating_element.find('img')['alt'] if review_rating_element and review_rating_element.find('img') else None

            # Extracting review content
            review_content_element = div.find('p', class_='typography_body-l__KUYFJ')
            review_content = review_content_element.text.strip() if review_content_element else None

            # Extracting date of experience
            date_of_experience_element = div.find('p', class_='typography_body-m__xgxZ_')
            date_of_experience = date_of_experience_element.text.strip() if date_of_experience_element else None



            # Adding company name (assuming 'Easy Auto.com' in this case)
            company_name = 'EasyAuto.com'

            # Create a new DataFrame with the current row
            new_row_df = pd.DataFrame({
                'Consumer Name': [consumer_name],
                'Review Rating': [review_rating],
                'Review Content': [review_content],
                'Date of Experience': [date_of_experience],
                'Company Name': [company_name]
            })

            # Concatenate the new DataFrame to the existing DataFrame
            df_company3 = pd.concat([df_company3, new_row_df], ignore_index=True)

        print(f'Page {page_num} scraped successfully.')
    else:
        print(f'Failed to retrieve data from page {page_num} even after retries.')

# Display the DataFrame
df_company3

"""**<h1>Data Transformation**

Data extraction from the 3 company had been done, now we will then do the data transformation/ extraction to all the dataset that we extract successfully.

**Data Transformation/ Cleaning for We Buy Any Car® USA**
"""

# Check for null values in the DataFrame
null_values = df_company1.isnull().sum()

# Display the null values count for each column
print("Null Values in Each Column:")
print(null_values)

df_company1 = df_company1.fillna("No Review")

# Check for null values in the filled DataFrame
null_values_filled = df_company1.isnull().sum()

# Display the null values count for each column
print("Null Values in Each Column:")
print(null_values_filled)

df_company1['Date of Experience'] = df_company1['Date of Experience'].str.replace('Date of experience: ', '')

# Display the updated DataFrame
df_company1

df_company1.index = range(1, len(df_company1) + 1)

# Extract the rating using a regular expression, format it as "X/5", and convert to float
df_company1['Review Rating'] = df_company1['Review Rating'].str.extract(r'(\d) out of 5 stars').astype(float) / 5

df_company1

"""Data Transformation/ Cleaning is done for the companny We Buy Any Car® USA. Where we have did:


*   Remove the null value in review content with 'No Review'
*   Transform the column "Date of Experience" to just the date and remove the word "Date of Experience"
*   Change the index from 0-1499 to 1-1500 for a easier view
*   Transform the review rating from Rated 5 out of 5 stars	to float value which is 1.0 where 1.0 = Rated 5 out of 5 stars.

**Data Transformation/ Cleaning for CarBrain**
"""

# Check for null values in the DataFrame
null_values = df_company2.isnull().sum()

# Display the null values count for each column
print("Null Values in Each Column:")
print(null_values)

df_company2 = df_company2.fillna("No Review")

# Check for null values in the filled DataFrame
null_values_filled = df_company2.isnull().sum()

# Display the null values count for each column
print("Null Values in Each Column:")
print(null_values_filled)

df_company2['Date of Experience'] = df_company2['Date of Experience'].str.replace('Date of experience: ', '')

# Display the updated DataFrame
df_company2

df_company2.index = range(1, len(df_company1) + 1)

# Extract the rating using a regular expression, format it as "X/5", and convert to float
df_company2['Review Rating'] = df_company2['Review Rating'].str.extract(r'(\d) out of 5 stars').astype(float) / 5

df_company2

"""Data Transformation/ Cleaning is done for the companny CarBrain. Where we have did:


*   Remove the null value in review content with 'No Review'
*   Transform the column "Date of Experience" to just the date and remove the word "Date of Experience"
*   Change the index from 0-1499 to 1-1500 for a easier view
*   Transform the review rating from Rated 4 out of 5 stars	to float value which is 0.8 where 1.0 = Rated 4 out of 5 stars.

**Data Transformation/ Cleaning for Easy Auto**
"""

# Check for null values in the DataFrame
null_values = df_company3.isnull().sum()

# Display the null values count for each column
print("Null Values in Each Column:")
print(null_values)

df_company3 = df_company3.fillna("No Review")

# Check for null values in the filled DataFrame
null_values_filled = df_company3.isnull().sum()

# Display the null values count for each column
print("Null Values in Each Column:")
print(null_values_filled)

df_company3['Date of Experience'] = df_company3['Date of Experience'].str.replace('Date of experience: ', '')

# Display the updated DataFrame
df_company3

df_company3.index = range(1, len(df_company3) + 1)

# Extract the rating using a regular expression, format it as "X/5", and convert to float
df_company3['Review Rating'] = df_company3['Review Rating'].str.extract(r'(\d) out of 5 stars').astype(float) / 5

df_company3

"""Data Transformation/ Cleaning is done for the companny EasyAuto. Where we have did:


*   Remove the null value in review content with 'No Review'
*   Transform the column "Date of Experience" to just the date and remove the word "Date of Experience"
*   Change the index from 0-1499 to 1-1500 for a easier view
*   Transform the review rating from Rated 2 out of 5 stars	to float value which is 0.4 where 0.4 = Rated 2 out of 5 stars.

**<h1>Data Visualization**

It is used to compare the 3 data set from the 3 companies, to determine which company perform better.

And we used the average rating to compare the performance of the 3 companies
"""

import matplotlib.pyplot as plt

# Calculate the average review rating for each company
average_rating_company1 = df_company1['Review Rating'].mean()
average_rating_company2 = df_company2['Review Rating'].mean()
average_rating_company3 = df_company3['Review Rating'].mean()

# Visualize the average ratings in a bar chart
company_names = ['We Buy Any Car® USA', 'Car Brain', 'Easy Auto']
average_ratings = [average_rating_company1, average_rating_company2, average_rating_company3]

plt.bar(company_names, average_ratings, color=['blue', 'green', 'orange'])
plt.title('Average Review Ratings by Company')
plt.xlabel('Company')
plt.ylabel('Average Rating')

# Annotate each bar with its corresponding average rating
for i, rating in enumerate(average_ratings):
    plt.text(i, rating, f'{rating:.2f}', ha='center', va='bottom')

plt.show()

"""From the graph above, we can conclude that the company We Buy Any Car® USA get the highest rating, 0.97 among the other 2 companies which are 0.96 and 0.93.

**From the company We Buy Any Car® USA, we will then do more data extraction from it.**
"""

df_company1

"""**Further Data Extraction on verified user only so that the data will be more reliable.**"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

# Initialize a DataFrame
df_company_verified = pd.DataFrame(columns=['Consumer Name', 'Review Rating', 'Review Content', 'Date of Experience', 'Company Name', "Verified", "Location"])

# Define the number of pages to scrape
num_pages = 75  # Adjust the number of pages based on your requirement

# Define headers to mimic a web browser
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# Define the maximum number of retries
max_retries = 2

# Loop through each page
for page_num in range(1, num_pages + 1):
    url = f'https://www.trustpilot.com/review/webuyanycarusa.com?page={page_num}&verified=true'

    retry_count = 0

    while retry_count < max_retries:
        # Add headers to the request
        page = requests.get(url, headers=headers)

        # Add a delay between requests to avoid rate limiting
        time.sleep(1)  # Adjust the delay as needed

        # Check if the request was successful
        if page.status_code == 200:
            break  # Break out of the retry loop if successful
        elif page.status_code == 403:
            # Increment the retry count
            retry_count += 1
            print(f'Retry {retry_count} - Got 403 Forbidden. Retrying...')
        else:
            print(f'Failed to retrieve data from page {page_num}. Status code: {page.status_code}')
            break  # Break out of the retry loop for other status codes

    if page.status_code == 200:
        soup = BeautifulSoup(page.text, 'html.parser')

        # Find all <div> elements with the specified class
        specific_divs = soup.find_all('div', class_='styles_cardWrapper__LcCPA styles_show__HUXRb styles_reviewCard__9HxJJ')

        # Iterate through each specific <div> and extract information
        for div in specific_divs:
            # Extracting consumer information
            # Extract Consumer Name
            consumer_name_element = div.find('span', class_='typography_heading-xxs__QKBS8')
            consumer_name = consumer_name_element.text.strip() if consumer_name_element else None

            # Extract Review Rating
            review_rating_element = div.find('div', class_='star-rating_starRating__4rrcf')
            review_rating = review_rating_element.find('img')['alt'] if review_rating_element and review_rating_element.find('img') else None

            # Extract Review Content
            review_content_element = div.find('p', class_='typography_body-l__KUYFJ')
            review_content = review_content_element.text.strip() if review_content_element else None

            # Extracting date of experience
            date_of_experience_element = div.find('p', class_='typography_body-m__xgxZ_')
            date_of_experience = date_of_experience_element.text.strip() if date_of_experience_element else None

            # Extract Verified User Status
            verified_user_element = div.find('button', {'data-review-label-tooltip-trigger': 'true'})
            is_verified_user = True if verified_user_element else False

            # Extract Location
            location_element = div.find('div', {'data-consumer-country-typography': 'true'})
            location = location_element.text.strip() if location_element else None


            # Adding company name (assuming 'webuyanycarusa.com' in this case)
            company_name = 'webuyanycarusa.com'

            # Create a new DataFrame with the current row
            new_row_df = pd.DataFrame({
                'Consumer Name': [consumer_name],
                'Review Rating': [review_rating],
                'Review Content': [review_content],
                'Date of Experience': [date_of_experience],
                'Company Name': [company_name],
                "Verified": [is_verified_user],
                "Location": [location]
            })

            # Concatenate the new DataFrame to the existing DataFrame
            df_company_verified = pd.concat([df_company_verified, new_row_df], ignore_index=True)

        print(f'Page {page_num} scraped successfully.')
    else:
        print(f'Failed to retrieve data from page {page_num} even after retries.')

# Display the DataFrame
df_company_verified

df_company_verified.to_csv('company_verified_data.csv', index=False)

"""**<h1>Data Cleaning**"""

# Check for null values in the DataFrame
null_values = df_company_verified.isnull().sum()

# Display the null values count for each column
print("Null Values in Each Column:")
print(null_values)

df_company_verified = df_company_verified.fillna("No Review")

# Check for null values in the filled DataFrame
null_values_filled = df_company_verified.isnull().sum()

# Display the null values count for each column
print("Null Values in Each Column:")
print(null_values_filled)

df_company_verified['Date of Experience'] = df_company_verified['Date of Experience'].str.replace('Date of experience: ', '')

# Display the updated DataFrame
df_company_verified

"""From the above, we had replace all the null values into "No Review" in the "Review Rating" column.

And we did remove the "Date of Experience" from the column "Date of Experience" and only remain the date

**<h1>Data Transformation**
"""

# Extract Numeric Rating
df_company_verified['Numeric Rating'] = df_company_verified['Review Rating'].str.extract('(\d+)').astype(float)

# Convert Date of Experience to Datetime
df_company_verified['Date of Experience'] = pd.to_datetime(df_company_verified['Date of Experience'], errors='coerce')

# Extract Year, Month, and Day
df_company_verified['Year'] = df_company_verified['Date of Experience'].dt.year
df_company_verified['Month'] = df_company_verified['Date of Experience'].dt.month
df_company_verified['Day'] = df_company_verified['Date of Experience'].dt.day

# Adjust Company Name
df_company_verified['Company Name'] = df_company_verified['Company Name'].str.replace('.com', '')

# Handle Location
df_company_verified['Location'] = df_company_verified['Location'].str.upper()  # Make sure all characters are uppercase

# Boolean to Numeric for Verified column
df_company_verified['Verified'] = df_company_verified['Verified'].astype(int)

# Display the cleaned DataFrame
df_company_verified

# Remove the 'Review Rating' column
df_company_verified = df_company_verified.drop('Review Rating', axis=1)

df_company_verified

df_company_verified.index = range(1, len(df_company_verified) + 1)

df_company_verified

"""From the data transformation above, we had did:


1.   Extract the review rating to numeric rating rather than "Rated 4 out of 5"
2.   Convert the date of experice column to datetime rather than 2023 January 20
3.   We extract the year, month, and day from into a new column from the date of experience column
4.   We adjust the company name by remove the ".com" and just the "webuycarusa"
5.   We also transform all the location into Upper case letter
6.  We also transform the data from verified column from True to 1
7.  We also change the index from 0-1499 to 1-1500.
8.  We also drop the review rating column since we ald have a new column names numeric rating.

**<h1>Data Visualization**
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Distribution of Review Ratings
plt.figure(figsize=(10, 6))
sns.histplot(df_company_verified['Numeric Rating'], bins=5, kde=False)
plt.title('Distribution of Review Ratings')
plt.xlabel('Numeric Rating')
plt.ylabel('Number of Reviews')
plt.show()

# Number of Reviews by Location
plt.figure(figsize=(12, 6))
ax = sns.countplot(x='Location', data=df_company_verified, palette='viridis', order=df_company_verified['Location'].value_counts().index)

# Annotate each bar with the count of reviews
offset = 10  # Adjust the offset as needed
for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height() + offset), ha='center', va='baseline', fontsize=8, color='black')

plt.title('Number of Reviews by Location')
plt.xlabel('Location')
plt.ylabel('Number of Reviews')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.show()

"""**<h3>Comparison of review contetnt between no data cleaning and after data cleaning**

In this, we will use **TextBlob** to convert the review content into real rating(numeric). But then we will compare the effect before and after data cleaning.

After that, we will use **data correlation** to compare with the numeric rating get from the each consumer to determine whether data cleaning do improve the convertion of review content into real rating(numeric) or not.
"""

from textblob import TextBlob

def text_to_normalized_rating(review_content):
    analysis = TextBlob(str(review_content))
    sentiment_polarity = analysis.sentiment.polarity

    # Normalize to the range of 0.0 to 5.0
    normalized_rating = (sentiment_polarity + 1) * 2.5

    # Round the result
    rounded_normalized_rating = round(normalized_rating, 1)
    return rounded_normalized_rating

# Apply the function to the 'Review Content' column and store the result in a new 'Normalized Rating' column
df_company_verified['Rating_from_Review_Content'] = df_company_verified['Review Content'].apply(text_to_normalized_rating)

# Display the DataFrame with the generated ratings
print(df_company_verified[['Review Content', 'Rating_from_Review_Content']])

"""The result above is the rating before data cleaning and is stored in a new column of Rating_from_Review_Content"""

import re
from textblob import TextBlob
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download NLTK resources (stop words)
import nltk
nltk.download('stopwords')
nltk.download('punkt')

# Function for text cleaning and normalization
def clean_and_normalize_text(review_content):
    # Convert to lowercase
    review_content = str(review_content).lower()

    # Remove special characters and digits
    review_content = re.sub(r'[^a-z\s]', '', review_content)

    # Tokenize and remove stop words
    stop_words = set(stopwords.words('english'))
    words = word_tokenize(review_content)
    filtered_words = [word for word in words if word not in stop_words]

    # Join the filtered words back into a string
    cleaned_text = ' '.join(filtered_words)

    # Sentiment analysis and rating calculation
    analysis = TextBlob(cleaned_text)
    sentiment_polarity = analysis.sentiment.polarity

    # Normalize to the range of 0.0 to 5.0
    normalized_rating = (sentiment_polarity + 1) * 2.5

    # Round the result
    rounded_normalized_rating = round(normalized_rating, 1)

    return rounded_normalized_rating

# Apply the function to the 'Review Content' column and store the result in a new 'Rating_from_Cleaned_Review' column
df_company_verified['Rating_from_Cleaned_Review'] = df_company_verified['Review Content'].apply(clean_and_normalize_text)

# Display the DataFrame with the generated ratings from cleaned review content
print(df_company_verified[['Review Content', 'Rating_from_Cleaned_Review']])

"""The result above is the rating before data cleaning and is stored in a new column of Rating_from_Cleaned_Review

Now we will then do data correlation to compare both rating (clean_review and original_review) with the numeric rating.
"""

correlation = df_company_verified['Rating_from_Review_Content'].corr(df_company_verified['Numeric Rating'])

print(f"Correlation between TextBlob rating and Numeric rating: {correlation}")

correlation = df_company_verified['Rating_from_Cleaned_Review'].corr(df_company_verified['Numeric Rating'])

print(f"Correlation between TextBlob rating and Numeric rating: {correlation}")

"""From the above, we can conclude that the correlation between both Rating_from_Review_Content and Rating_from_Cleaned_Review with with the numeric rating is that the Rating_from_Review_Content is performing better than the Rating_from_Cleaned_Review whcih is 0.17 and 0.14 respectively.

Reasons of why unclean data perform better:


1.   Sensitivity to Language Nuances: TextBlob's sentiment analysis may not capture all the nuances of language. It might struggle with sarcasm, irony, or other subtleties present in reviews.
2.   Subjectivity vs. Polarity: TextBlob provides both a polarity and a subjectivity score. Polarity measures the sentiment (positive/negative), while subjectivity measures the degree of subjectivity.
3. Fine-tuning Parameters: TextBlob may have parameters that can be fine-tuned.
4. Ground Truth Evaluation: Compare the TextBlob predictions with the ground truth (actual ratings) to understand where the model might be making errors.
"""

average_sentiment_rating = df_company_verified['Rating_from_Review_Content'].mean()
average_cleaned_rating = df_company_verified['Rating_from_Cleaned_Review'].mean()

# Print the averages
print(f"Average Sentiment Rating: {average_sentiment_rating}")
print(f"Average Cleaned Rating: {average_cleaned_rating}")

"""From the above, we find the mean rating from both clean and not clean data.

Althought the correlation of both rating is not close as the numeric rating, but in another way, we also can be conclude that from the clean sentiment rating, the rating is just about 3.32 but not totally perform as good as 5.00 that show in the graph above.

It is because that sometimes rating in the website is not totally trustable, but the comment from the user also can be used to justify whether the rating is reliable or not.
"""

df_company_verified.to_csv('company_verified_data.csv', index=False)